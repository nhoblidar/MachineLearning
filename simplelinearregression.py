# -*- coding: utf-8 -*-
"""SimpleLinearRegression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11PMQwjTgZcmFoPHIe4J79Zd7cxLiD1M4

#Linear Regression

###Linear Regression was one of the very first machine learning algorithms I learned, and it laid the foundation for understanding many others.

This algorithm assumes that the features (inputs) and target (output) have a linear relationship.

In this project, I demonstrate Linear Regression with a single variable:

Input (feature): House size in square feet

Output (target): House price (in $1000s)

The code walks through the step-by-step process of training the model, visualizing results, and understanding how the line of best fit is derived.
"""

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

from google.colab import files
uploaded = files.upload()

# Commented out IPython magic to ensure Python compatibility.
#Importng necessary library
#Numpy, a popular library for scientific computing
#Matplotlib, a popular library for plotting data
#plotting routines in the lab_utils.py file
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
from lab_utils_uni import plt_intuition, plt_stationary, plt_update_onclick, soup_bowl, plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients
plt.style.use('./deeplearning.mplstyle')

"""First for simplicity the data set with only two data points - a house with 1000 square feet(sqft) sold for $300,000 and a house with 2000 square feet sold for 500,000. These two points will constitute the data or training set. In this code, the units of size are 1000 sqft and the units of price are 1000s of dollars."""

# x_train is the input variable (size in 1000 square feet)
# y_train is the target (price in 1000s of dollars)
x_train = np.array([1.0,2.0])
y_train = np.array([300.0,500.0])
print(f"x_train = {x_train}")
print(f"y_train = {y_train}")

# m is the number of training examples
print(f"x_train.shape: {x_train.shape}")
m = x_train.shape[0]
print(f"Number of training examples is: {m}")

"""Plotting the data

"""

plt.scatter(x_train, y_train , marker ='x', c = 'red')
plt.title("Housing Prices")
plt.ylabel('Price (in 1000s of dollars)')
plt.xlabel('Size (1000 sqft)')

"""The model function for linear regression (which is a function that maps from x to y) is represented as

$$ f_{w,b}(x^{(i)}) = wx^{(i)} + b \tag{1}$$

The formula above is how we can represent straight lines - different values of  ùë§ and  ùëè give you different straight lines on the plot.

Let's try to get a better intuition for this through the code blocks below. Let's start with $w = 100$ and $b = 100$.
"""

w = 100
b = 100

"""Now, let's compute the value of $f_{w,b}(x^{(i)})$ for your two data points. You can explicitly write this out for each data point as -

for $x^{(0)}$, `f_wb = w * x[0] + b`

for $x^{(1)}$, `f_wb = w * x[1] + b`

For a large number of data points, this can get unwieldy and repetitive. So instead, you can calculate the function output in a `for` loop as shown in the `compute` function below.

"""

def compute(x,w,b):
 m = len(x_train)
 f_wb = np.zeros(m)
 for i in range(m):
   f_wb[i] = w*x[i]+b
 return f_wb

result = compute(x_train, w, b)

print(result)

"""As you can see, setting $w = 100$ and $b = 100$ does *not* result in a line that fits our data."""

plt.scatter(x_train, y_train, marker = 'x', color = 'blue', label = 'Actual Prediction')
plt.plot(x_train, result, c = 'red', label = "Our Prediction")
plt.title("Housing Prices")
plt.ylabel('Price (in 1000s of dollars)')
plt.xlabel('Size (1000 sqft)')
plt.legend()
plt.show()

"""The goal is to find the optimal values of w (weight) and b (bias) that minimize the error between the predicted values and the actual target values. So let's try different values for w and b"""

w = 200
b = 100

result = compute(x_train, w, b)

print(result)

"""Yes now the optimal values of w (weight) and b (bias) are determined that minimize the error between our predictions and the actual target values, giving us the best-fit line for the data."""

plt.scatter(x_train, y_train, marker = 'x', color = 'blue', label = 'Actual Prediction')
plt.plot(x_train, result, c = 'red', label = "Our Prediction")
plt.title("Housing Prices")
plt.ylabel('Price (in 1000s of dollars)')
plt.xlabel('Size (1000 sqft)')
plt.legend()
plt.show()

"""# Understanding Cost Function in Linear Regression

## The Problem
We want to fit a line to our data:

\[
y = wx + b
\]

- **w** = slope (controls steepness of the line)  
- **b** = intercept (where the line crosses the y-axis)

---

## Why Not Just Pick w and b Manually?
If we had only a few data points, we could adjust `w` and `b` by hand.  
But with thousands of rows, manual tuning is **impossible**.

---

## The Solution: Cost Function
The **cost function** tells us how well the line fits the data.  
We want to make this value as **small as possible**.

The most common cost function in regression is **Mean Squared Error (MSE):**

  $$J(w,b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \tag{1}$$

  where
  $$f_{w,b}(x^{(i)}) = wx^{(i)} + b \tag{2}$$
  
- $f_{w,b}(x^{(i)})$ is our prediction for example $i$ using parameters $w,b$.  
- $(f_{w,b}(x^{(i)}) -y^{(i)})^2$ is the squared difference between the target value and the prediction.   
- These differences are summed over all the $m$ examples and divided by `2m` to produce the cost, $J(w,b)$.  

---

##  Optimization
Instead of manually changing `w` and `b`, we use **Gradient Descent**:

- Start with random `w` and `b`  
- Take small steps to reduce the cost  
- Repeat until the cost function is minimized  

Imagine standing on top of a hill (high cost).  
Gradient Descent is like walking **downhill step by step** until you reach the valley (low cost).

"""

def compute_cost(x, y, w, b):
    m = x_train.shape[0]
    cost_sum = 0
    for i in range(m):
        f_wb = w * x[i] + b
        cost = (f_wb - y[i])**2
        cost_sum = cost_sum + cost

    total_cost = cost_sum/ (2*m)
    return total_cost

"""The goal is to find a model $f_{w,b}(x) = wx + b$, with parameters $w,b$,  which will accurately predict house values given an input $x$. The cost is a measure of how accurate the model is on the training data.

The cost equation (1) above shows that if $w$ and $b$ can be selected such that the predictions $f_{w,b}(x)$ match the target data $y$, the $(f_{w,b}(x^{(i)}) - y^{(i)})^2 $ term will be zero and the cost minimized. In this simple two point example, you can achieve this!

In the previous section, it was determined that $b=100$ provided an optimal solution so let's set $b$ to 100 and focus on $w$.

<br/>
Below, see the slider control to select the value of $w$ that minimizes cost. It can take a few seconds for the plot to update.
"""

plt_intuition(x_train,y_train)

"""The plot contains a few points that are worth mentioning.
- cost is minimized when $w = 200$, which matches results from the previous lab
- Because the difference between the target and pediction is squared in the cost equation, the cost increases rapidly when $w$ is either too large or too small.
- Using the `w` and `b` selected by minimizing cost results in a line which is a perfect fit to the data.

### Larger Data Set
Including some more data points will give a better intution
"""

x1_train = np.array([1.0, 1.7, 2.0, 2.5, 3.0, 3.2])
y1_train = np.array([250, 300, 480,  430,   630, 730,])

"""In the contour plot, by clicking on a point to select `w` and `b` we can achieve the lowest cost."""

plt.close('all')
fig, ax, dyn_items = plt_stationary(x1_train, y1_train)

updater = plt_update_onclick(fig, ax, x1_train, y1_train, dyn_items)

"""Above, note the dashed lines in the left plot. These represent the portion of the cost contributed by each example in your training set. In this case, values of approximately $w=209$ and $b=2.4$ provide low cost.

### Convex Cost surface
The fact that the cost function squares the loss ensures that the 'error surface' is convex like a soup bowl. It will always have a minimum that can be reached by following the gradient in all dimensions. In the previous plot, because the $w$ and $b$ dimensions scale differently, this is not easy to recognize.
"""

soup_bowl()

"""## Gradient Descent
Now to minimize the cost function Gradient Descent does the job

So far there is a linear model that predicts $f_{w,b}(x^{(i)})$:
$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \tag{1}$$

In linear regression, input training data is utilized to fit the parameters $w$,$b$ by minimizing a measure of the error between our predictions $f_{w,b}(x^{(i)})$ and the actual data $y^{(i)}$. The measure is called the $cost$, $J(w,b)$. In training measure the cost over all of our training samples $x^{(i)},y^{(i)}$
$$J(w,b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\tag{2}$$

Now using Gradient Descent

$$\begin{align*} \text{repeat}&\text{ until convergence:} \; \lbrace \newline
\;  w &= w -  \alpha \frac{\partial J(w,b)}{\partial w} \tag{3}  \; \newline
 b &= b -  \alpha \frac{\partial J(w,b)}{\partial b}  \newline \rbrace
\end{align*}$$
where, parameters $w$, $b$ are updated simultaneously.  
The gradient is defined as:
$$
\begin{align}
\frac{\partial J(w,b)}{\partial w}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \tag{4}\\
  \frac{\partial J(w,b)}{\partial b}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \tag{5}\\
\end{align}
$$

Here simultaneously calculate the partial derivatives for all the parameters before updating any of the parameters.
"""

def compute_gradient(x, y, w, b):
  m = x_train.shape[0]
  dj_dw = 0
  dj_db = 0

  for i in range(m):
    f_wb = w * x[i] + b
    dj_dw_i = (f_wb - y[i]) * x[i]
    dj_db_i = f_wb - y[i]
    dj_dw += dj_dw_i
    dj_db += dj_db_i
  dj_dw = dj_dw / m
  dj_db = dj_db / m

  return dj_dw, dj_db

plt_gradients(x_train,y_train, compute_cost, compute_gradient)
plt.show()

"""Above, the left plot shows $\frac{\partial J(w,b)}{\partial w}$ or the slope of the cost curve relative to $w$ at three points. On the right side of the plot, the derivative is positive, while on the left it is negative. Due to the 'bowl shape', the derivatives will always lead gradient descent toward the bottom where the gradient is zero.

The left plot has fixed $b=100$. Gradient descent will utilize both $\frac{\partial J(w,b)}{\partial w}$ and $\frac{\partial J(w,b)}{\partial b}$ to update parameters. The 'quiver plot' on the right provides a means of viewing the gradient of both parameters. The arrow sizes reflect the magnitude of the gradient at that point. The direction and slope of the arrow reflects the ratio of $\frac{\partial J(w,b)}{\partial w}$ and $\frac{\partial J(w,b)}{\partial b}$ at that point.
Note that the gradient points *away* from the minimum. Review equation (3) above. The scaled gradient is *subtracted* from the current value of $w$ or $b$. This moves the parameter in a direction that will reduce cost.

Why don't we check cost and update w and b instead of defining the iterations?
We don‚Äôt check cost at every step because:

Gradient descent already ensures updates reduce cost (for reasonable
ùõº).

Checking would make training much slower.

Instead, we log cost at intervals just to monitor convergence, not to decide whether to update.
"""

def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function):

  J_history = []
  p_history = []
  w = w_in
  b = b_in

  for i in range(num_iters):
    dj_dw, dj_db = gradient_function(x, y, w, b)

    w = w - alpha * dj_dw
    b = b - alpha * dj_db

    if i < 100000:
      J_history.append(cost_function(x, y, w, b))
      p_history.append([w,b])

    if i % math.ceil(num_iters/10) ==  0:
      print(f"Iteration {i:4}: Cost {J_history[-1]:0.2e} ",
            f"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e} ",
            f"w: {w: 0.3e}, b:{b: 0.5e}")
  return w, b, J_history, p_history

import math

w_init = 0
b_init = 0
iterations = 10000
tmp_alpha = 1.0e-2

w_final, b_final, J_hist, p_hist = gradient_descent(x_train, y_train, w_init, b_init, tmp_alpha, iterations, compute_cost, compute_gradient)

print(f"(w,b) found by gradient descent: ({w_final:0.2f},{b_final:0.2f})")

"""- The cost starts large and rapidly declines as described in the slide from the lecture.
- The partial derivatives, `dj_dw`, and `dj_db` also get smaller, rapidly at first and then more slowly. As shown in the diagram from the lecture, as the process nears the 'bottom of the bowl' progress is slower due to the smaller value of the derivative at that point.
- progress slows though the learning rate, alpha, remains fixed

####Cost versus iterations of gradient descent
A plot of cost versus iterations is a useful measure of progress in gradient descent. Cost should always decrease in successful runs. The change in cost is so rapid initially, it is useful to plot the initial decent on a different scale than the final descent. In the plots below, note the scale of cost on the axes and the iteration step.
"""

fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))
ax1.plot(J_hist[:100])
ax2.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:])
ax1.set_title("Cost vs. iteration(start)");  ax2.set_title("Cost vs. iteration (end)")
ax1.set_ylabel('Cost')            ;  ax2.set_ylabel('Cost')
ax1.set_xlabel('iteration step')  ;  ax2.set_xlabel('iteration step')
plt.show()

"""### Predictions
Now that have discovered the optimal values for the parameters $w$ and $b$, let's use the model to predict housing values based on learned parameters. As expected, the predicted values are nearly the same as the training values for the same housing. Further, the value not in the prediction is in line with the expected value.
"""

print(f"1000 sqft house prediction {w_final*1.0 + b_final:0.1f} Thousand dollars")
print(f"1200 sqft house prediction {w_final*1.2 + b_final:0.1f} Thousand dollars")
print(f"2000 sqft house prediction {w_final*2.0 + b_final:0.1f} Thousand dollars")

"""## Plotting
The progress of gradient descent during its execution by plotting the cost over iterations on a contour plot of the cost(w,b).
"""

fig, ax = plt.subplots(1,1, figsize=(12, 6))
plt_contour_wgrad(x_train, y_train, p_hist, ax)

"""Above, the contour plot shows the $cost(w,b)$ over a range of $w$ and $b$. Cost levels are represented by the rings. Overlayed, using red arrows, is the path of gradient descent. Here are some things to note:
- The path makes steady (monotonic) progress toward its goal.
- initial steps are much larger than the steps near the goal.

**Zooming in**, we can see that final steps of gradient descent. Note the distance between steps shrinks as the gradient approaches zero.
"""

fig, ax = plt.subplots(1,1, figsize=(12, 4))
plt_contour_wgrad(x_train, y_train, p_hist, ax, w_range=[180, 220, 0.5], b_range=[80, 120, 0.5],
            contours=[1,5,10,20],resolution=0.5)

"""
### Increased Learning Rate

 The larger $\alpha$ is, the faster gradient descent will converge to a solution. But, if it is too large, gradient descent will diverge.

Let's try increasing the value of  $\alpha$ and see what happens:
"""

w_init = 0
b_init = 0
iterations = 10
tmp_alpha = 8.0e-1
w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha,
                                                    iterations, compute_cost, compute_gradient)

"""Above, $w$ and $b$ are bouncing back and forth between positive and negative with the absolute value increasing with each iteration. Further, each iteration $\frac{\partial J(w,b)}{\partial w}$ changes sign and cost is increasing rather than decreasing. This is a clear sign that the *learning rate is too large* and the solution is diverging.
Let's visualize this with a plot.
"""

plt_divergence(p_hist, J_hist,x_train, y_train)
plt.show()

"""Above, the left graph shows $w$'s progression over the first few steps of gradient descent. $w$ oscillates from positive to negative and cost grows rapidly. Gradient Descent is operating on both $w$ and $b$ simultaneously, so one needs the 3-D plot on the right for the complete picture.

##Linear Regression using Scikit-learn library

####Using scikit-learn, implementing linear regression becomes extremely simple and efficient, as the library provides built-in functions for the entire process. Instead of manually writing the gradient descent or normal equation steps, you can fit a model with just a few lines of code.

1. Basic Regression

In sklearn we have linear_model which has the following versions

LinearRegression ‚Üí Ordinary Least Squares (classic regression).

Ridge ‚Üí Linear regression with L2 regularization (helps prevent overfitting).

Lasso ‚Üí Linear regression with L1 regularization (forces some coefficients to zero ‚Üí feature selection).

ElasticNet ‚Üí Combination of L1 (Lasso) and L2 (Ridge).

BayesianRidge ‚Üí Bayesian version of regression, gives uncertainty estimates.

ARDRegression ‚Üí Automatic Relevance Determination (Bayesian method that prunes irrelevant features).

HuberRegressor ‚Üí Robust regression (less sensitive to outliers).

RANSACRegressor ‚Üí Fits only on inliers, ignores outliers.

TheilSenRegressor ‚Üí Robust to outliers, works well with small data.

PoissonRegressor ‚Üí For count data (target variable is counts).

GammaRegressor ‚Üí For strictly positive, skewed target data.

TweedieRegressor ‚Üí Generalized model that covers Poisson, Gamma, and Normal distributions.


####Linear Models for Online / Large-scale Learning

These are optimized for big datasets:

SGDRegressor ‚Üí Linear regression with stochastic gradient descent.

PassiveAggressiveRegressor ‚Üí Works well in streaming/online learning.

Scikit-learn has linear regression, regularized regressions (Ridge, Lasso, ElasticNet), robust regressions (Huber, RANSAC), probabilistic regressions (Bayesian, Poisson, Gamma, Tweedie), and linear classifiers (Logistic, Perceptron, SGD, PassiveAggressive, RidgeClassifier).

Out of this most used is LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor

###X must be 2D and y can be 1D or 2D


####Why X must be 2D

In scikit-learn:

X = features / input variables

Shape: [n_samples, n_features]

So, even if you have just one feature, scikit-learn still expects a 2D structure.

Each row = one data point (sample).

Each column = one feature.



---



####Why y can be 1D or 2D  

y = target/output variable.

For normal regression/classification, we usually have one target per sample.

Shape = [n_samples] (1D array).

y is 1D if single target, 2D if multiple targets.
"""

from sklearn.linear_model import LinearRegression
import numpy as np

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

x_train = np.array([[1.0],[2.0]])
y_train = np.array([300.0,500.0])

model = LinearRegression(fit_intercept = True)
model.fit(x_train, y_train)

print("Coef:", model.coef_)
print("Intercept:", model.intercept_)

"""Now with few lines of code we got the weight and bias which we earliar got with gradient descent"""

y_pred = model.predict(x_train)
print(y_pred)

"""Evaluation with y_train but usually after fitting the data we can evaluate with the y_test and see the results. R^2 score, MSE or MAE can be used for evaluation"""

print("R¬≤ Score:", r2_score(y_train, y_pred))
print("MSE:", mean_squared_error(y_train, y_pred))
print("MAE:", mean_absolute_error(y_train, y_pred))

x_test = np.array([[1.0], [1.2], [2.0]])
y_test = model.predict(x_test)
print(y_test)

"""### Drawbacks of Linear Regression

The drawback of Linear Regression is that it assumes the features are linearly dependent on the target variable. If the features affect the target in a non-linear way, need feature engineering or more flexible models.

1. **Cannot capture non-linear relationships**  
   - Linear Regression assumes a linear relationship between features and target.  
   - If the true relationship is curved, quadratic, exponential, or more complex, the model will **underfit**.  
   - Example:  
     \[
     y = x^2
     \]  
     A linear model will only try to approximate this with a straight line ‚Üí poor predictions.

2. **Sensitive to outliers**  
   - Linear Regression minimizes **squared errors**, so extreme values can heavily skew the model.

3. **Multicollinearity issues**  
   - When features are highly correlated, the estimated coefficients become **unstable** and less reliable.

4. **Limited flexibility**  
   - Works well only when the data approximately follows a linear pattern.  
   - Cannot automatically model interactions or non-linear effects without feature engineer

"""